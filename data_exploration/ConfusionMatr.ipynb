{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUrn8O3DACP4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio \\\n",
        "            tqdm numpy opencv-python \\\n",
        "            transformers matplotlib \\\n",
        "            albumentations Pillow \\\n",
        "            scikit-learn seaborn \\\n",
        "            torchmetrics evaluate \\\n",
        "            gdown"
      ],
      "metadata": {
        "id": "iyWWrk4jwuNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import TimesformerForVideoClassification, AutoImageProcessor, AutoFeatureExtractor, AutoModelForVideoClassification, TrainingArguments, Trainer\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "import evaluate\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import gdown\n",
        "import zipfile\n"
      ],
      "metadata": {
        "id": "YO0dphQGs8b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "JnCSx0VQxLj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading dataset\n",
        "\n",
        "# Google Drive file ID\n",
        "file_id = \"1BqMBtsuvb6mTpiZUZ9WKcJA8f1hkI2yX\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Download file\n",
        "output = \"HMDB_simp.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "print(\"Download and extraction complete!\")"
      ],
      "metadata": {
        "id": "ezWcNB66APln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORY_INDEX = {\n",
        "    \"brush_hair\": 0,\n",
        "    \"cartwheel\": 1,\n",
        "    \"catch\": 2,\n",
        "    \"chew\": 3,\n",
        "    \"climb\": 4,\n",
        "    \"climb_stairs\": 5,\n",
        "    \"draw_sword\": 6,\n",
        "    \"eat\": 7,\n",
        "    \"fencing\": 8,\n",
        "    \"flic_flac\": 9,\n",
        "    \"golf\": 10,\n",
        "    \"handstand\": 11,\n",
        "    \"kiss\": 12,\n",
        "    \"pick\": 13,\n",
        "    \"pour\": 14,\n",
        "    \"pullup\": 15,\n",
        "    \"pushup\": 16,\n",
        "    \"ride_bike\": 17,\n",
        "    \"shoot_bow\": 18,\n",
        "    \"shoot_gun\": 19,\n",
        "    \"situp\": 20,\n",
        "    \"smile\": 21,\n",
        "    \"smoke\": 22,\n",
        "    \"throw\": 23,\n",
        "    \"wave\": 24,\n",
        "}\n"
      ],
      "metadata": {
        "id": "DM6apGPhp9CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation: Resize to 224x224 and Convert to Tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def load_sampled_frames(frame_dir, frame_rate=8):\n",
        "    \"\"\"\n",
        "    Load every [frame_rate]-th frame from a directory and apply transformations.\n",
        "    \"\"\"\n",
        "    frame_files = sorted(os.listdir(frame_dir))  # Ensure frames are in order\n",
        "    sampled_frames = []\n",
        "\n",
        "    for i in range(0, len(frame_files), frame_rate):\n",
        "        frame_path = os.path.join(frame_dir, frame_files[i])\n",
        "        frame = Image.open(frame_path).convert(\"RGB\")  # Convert to RGB\n",
        "        frame = transform(frame)  # Apply transformations\n",
        "        sampled_frames.append(frame)\n",
        "\n",
        "    return sampled_frames  # List of torch tensors\n",
        "\n",
        "def create_clips(frames, clip_size=8):\n",
        "    \"\"\"\n",
        "    Given a list of sampled frames, create multiple [clip_size]-frame clips.\n",
        "    \"\"\"\n",
        "    clips = []\n",
        "    if len(frames) < clip_size:\n",
        "        return clips  # Not enough frames to create a clip\n",
        "    for i in range(0, len(frames) - clip_size + 1, clip_size):  # Sliding window\n",
        "        clip = torch.stack(frames[i:i + clip_size])  # Stack into (clip_size, 3, 224, 224)\n",
        "        clips.append(clip)\n",
        "\n",
        "    return clips"
      ],
      "metadata": {
        "id": "P--_dzefBom3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/HMDB_simp\"\n",
        "\n",
        "import random\n",
        "\n",
        "def split_sources(dataset_path, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Splits source folders into train and val sets before processing clips.\n",
        "    Ensures that all clips from a source video stay in the same set.\n",
        "    \"\"\"\n",
        "    train_sources = {}\n",
        "    val_sources = {}\n",
        "\n",
        "    for category in os.listdir(dataset_path):  # Iterate over action categories\n",
        "        category_path = os.path.join(dataset_path, category)\n",
        "        if not os.path.isdir(category_path):\n",
        "            continue\n",
        "\n",
        "        instances = os.listdir(category_path)  # List all source folders (video IDs)\n",
        "        random.shuffle(instances)  # Shuffle instances before splitting\n",
        "\n",
        "        split_idx = int(len(instances) * train_ratio)\n",
        "        train_sources[category] = instances[:split_idx]  # First 80% for training\n",
        "        val_sources[category] = instances[split_idx:]  # Last 20% for validation\n",
        "\n",
        "    return train_sources, val_sources\n",
        "\n",
        "\n",
        "def process_dataset(dataset_path, sources_dict):\n",
        "    \"\"\"\n",
        "    Processes dataset based on a predefined list of sources.\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "\n",
        "    for category, instances in tqdm(sources_dict.items()):\n",
        "        category_path = os.path.join(dataset_path, category)\n",
        "\n",
        "        for instance in instances:\n",
        "            instance_path = os.path.join(category_path, instance)\n",
        "            if not os.path.isdir(instance_path):\n",
        "                continue  # Skip non-directory files\n",
        "\n",
        "\n",
        "            # Load sampled frames\n",
        "            frames = load_sampled_frames(instance_path)\n",
        "\n",
        "            # Create 8-frame clips\n",
        "            clips = create_clips(frames)\n",
        "\n",
        "            for clip in clips:\n",
        "                dataset.append((clip, CATEGORY_INDEX[category]))  # Store (clip, label)\n",
        "\n",
        "    return dataset  # List of (clip, label)\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip, label = self.dataset[idx]\n",
        "        return clip, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "from torch.utils.data import default_collate\n",
        "\n",
        "class VideoDataCollator:\n",
        "    \"\"\"\n",
        "    Custom data collator for TimeSFormer.\n",
        "    Converts (clip, label) tuples into a dictionary format.\n",
        "    \"\"\"\n",
        "    def __call__(self, features):\n",
        "        clips, labels = zip(*features)  # Unpack (clip, label)\n",
        "        batch = {\n",
        "            \"pixel_values\": torch.stack(clips),  # Stack clips into batch\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long)  # Convert labels to tensor\n",
        "        }\n",
        "        return batch\n"
      ],
      "metadata": {
        "id": "EZ5uc9RyCaP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split source folders into train & val\n",
        "train_sources, val_sources = split_sources(DATASET_PATH)\n",
        "\n",
        "# Process train and val sets separately\n",
        "train_dataset = process_dataset(DATASET_PATH, train_sources)\n",
        "val_dataset = process_dataset(DATASET_PATH, val_sources)\n",
        "\n",
        "dataset_size = len(train_dataset) + len(val_dataset)\n",
        "\n",
        "print(f\"Total clips: {dataset_size}, Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dWD2A1WTCTO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "S2Jrj0STIKLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = AutoFeatureExtractor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "model = AutoModelForVideoClassification.from_pretrained(\n",
        "    \"facebook/timesformer-base-finetuned-k400\",\n",
        "    num_labels=len(CATEGORY_INDEX),  # Adjust for our dataset\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "# Send model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model on: {device}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FSbE-vwYICqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "top5_metric = torchmetrics.classification.Accuracy(top_k=5, task=\"multiclass\", num_classes=len(CATEGORY_INDEX)).to(device)\n",
        "\n",
        "# Metrics Function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.tensor(logits).argmax(dim=1)\n",
        "\n",
        "    # Compute Accuracy\n",
        "    top1_acc = accuracy_metric.compute(predictions=predictions.numpy(), references=labels)[\"accuracy\"]\n",
        "\n",
        "    # Compute Top-5 Accuracy\n",
        "    top5_acc = top5_metric(torch.tensor(logits).to(device), torch.tensor(labels).to(device)).item()\n",
        "\n",
        "    # Compute F1-score (macro)\n",
        "    f1 = f1_metric.compute(predictions=predictions.numpy(), references=labels, average=\"macro\")[\"f1\"]\n",
        "\n",
        "    # Confusion Matrix\n",
        "    INDEX_CATEGORY = {v: k for k, v in CATEGORY_INDEX.items()}\n",
        "    cm = confusion_matrix(labels, predictions, normalize=\"true\",labels=list(CATEGORY_INDEX.values()))\n",
        "\n",
        "    # Plot confusion matrix with labeled axes\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.set(font_scale=0.5)\n",
        "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap='Blues',\n",
        "            xticklabels=list(CATEGORY_INDEX.keys()),\n",
        "            yticklabels=list(CATEGORY_INDEX.keys()))\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": top1_acc,\n",
        "        \"top-5 accuracy\": top5_acc,\n",
        "        \"f1-score\": f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "8VFncTpKGeBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./timesformer_output\",  # Save checkpoints\n",
        "    eval_strategy=\"epoch\",  # Evaluate after every epoch\n",
        "    save_strategy=\"epoch\",  # Save model after each epoch\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",  # TensorBoard logs\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,  # Keep only last 2 checkpoints\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "data_collator = VideoDataCollator()\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=extractor,  # Feature extractor\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train Model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Jn46WOV_TF6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiments with a trained model.\n",
        "\n",
        "Extract files from the timesformer_output.zip archive to the left bar on the content level. Model.safetensors and optimizer.pt may take a bit longer to be uploaded."
      ],
      "metadata": {
        "id": "sJezYu6aeX1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading our trained model\n",
        "model = AutoModelForVideoClassification.from_pretrained(\n",
        "    \"/content/\",  # Path where model.safetensors and config.json are\n",
        "    num_labels=25,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# validation dataset loader\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)\n",
        "\n",
        "# just inference\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for videos, labels in val_loader:\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(videos)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        print(preds)\n",
        "        print(labels)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())"
      ],
      "metadata": {
        "id": "rbnAxb52IOFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zPJukHA8xIYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GdTHLs0NMjXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}