{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUrn8O3DACP4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy opencv-python transformers matplotlib albumentations gdown"
      ],
      "metadata": {
        "id": "L2AIa4t2z-Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "#!pip install transformers torch torchvision datasets evaluate torchmetrics\n",
        "\n",
        "# Google Drive file ID\n",
        "file_id = \"1BqMBtsuvb6mTpiZUZ9WKcJA8f1hkI2yX\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Download file\n",
        "output = \"HMDB_simp_clean.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "print(\"Download and extraction complete!\")"
      ],
      "metadata": {
        "id": "ezWcNB66APln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from transformers import TimesformerForVideoClassification, AutoImageProcessor\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A"
      ],
      "metadata": {
        "id": "SuM3xFxejo_q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        #A.Normalize(mean=[0.5, 0.5,0.5], std=[0.5, 0.5, 0.5]),\n",
        "        A.ToTensorV2(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "j5SQwmHTjsYc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORY_INDEX = {\n",
        "    \"brush_hair\": 0,\n",
        "    \"cartwheel\": 1,\n",
        "    \"catch\": 2,\n",
        "    \"chew\": 3,\n",
        "    \"climb\": 4,\n",
        "    \"climb_stairs\": 5,\n",
        "    \"draw_sword\": 6,\n",
        "    \"eat\": 7,\n",
        "    \"fencing\": 8,\n",
        "    \"flic_flac\": 9,\n",
        "    \"golf\": 10,\n",
        "    \"handstand\": 11,\n",
        "    \"kiss\": 12,\n",
        "    \"pick\": 13,\n",
        "    \"pour\": 14,\n",
        "    \"pullup\": 15,\n",
        "    \"pushup\": 16,\n",
        "    \"ride_bike\": 17,\n",
        "    \"shoot_bow\": 18,\n",
        "    \"shoot_gun\": 19,\n",
        "    \"situp\": 20,\n",
        "    \"smile\": 21,\n",
        "    \"smoke\": 22,\n",
        "    \"throw\": 23,\n",
        "    \"wave\": 24,\n",
        "}"
      ],
      "metadata": {
        "id": "BUn_ujQRjvUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To access the trained model, please extract the attached transformer_output files and specify the directory to it."
      ],
      "metadata": {
        "id": "QaMREGOn2wVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# targeted  video\n",
        "video_folder = \"/content/HMDB_simp_clean/brush_hair/F24EB2B7\"\n",
        "# targeted  class\n",
        "CLASS = \"brush_hair\"\n",
        "target_category = CATEGORY_INDEX[CLASS]\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# loading trained model\n",
        "model_dir = \"/content/\" # path where model.safetensors, optimizer.pt, etc are located\n",
        "model = TimesformerForVideoClassification.from_pretrained(model_dir).to(device)\n",
        "processor = AutoImageProcessor.from_pretrained(model_dir)"
      ],
      "metadata": {
        "id": "SwKRVtZMj1Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chosen target layer which worked best\n",
        "target_layer = model.timesformer.encoder.layer[-1].attention.attention\n",
        "\n",
        "activations = None\n",
        "gradients = None\n",
        "\n",
        "\n",
        "def save_activation_hook(module, input, output):\n",
        "    global activations\n",
        "    activations = output[0].detach()\n",
        "\n",
        "\n",
        "def save_gradient_hook(module, grad_input, grad_output):\n",
        "    global gradients\n",
        "    gradients = grad_output[0].detach()\n",
        "\n",
        "\n",
        "target_layer.register_forward_hook(save_activation_hook)\n",
        "target_layer.register_full_backward_hook(save_gradient_hook)\n",
        "\n",
        "\n",
        "def load_frames(folder_path):\n",
        "    frame_files = sorted(os.listdir(folder_path))\n",
        "    original_frames = []\n",
        "    frames = []\n",
        "    assert len(frame_files) >= 8 * 8\n",
        "    for file in frame_files[:8*8:8]:\n",
        "        img_path = os.path.join(folder_path, file)\n",
        "        frame = cv2.imread(img_path)\n",
        "        frame = cv2.resize(frame, (224, 224))\n",
        "        # Convert to grayscale but keep 3 dimensions for RGB compatibility\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        rgb_compatible_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2BGR)\n",
        "        original_frames.append(rgb_compatible_frame)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = default_transforms(image=frame)[\"image\"]\n",
        "        frames.append(frame)\n",
        "    frames = torch.stack(frames)  # (8, 3, 224, 224)\n",
        "    return frames, original_frames\n",
        "\n",
        "\n",
        "clip, original_frames = load_frames(video_folder)  # video tensor (8, 3, 224, 224)\n",
        "inputs = clip.unsqueeze(0).to(device).float()  # matching size - (1, 8, 3, 224, 224)\n",
        "inputs = {\"pixel_values\": inputs}\n",
        "\n",
        "# Forward pass to get class scores\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "\n",
        "# Backpropagate only the target class to compute gradients\n",
        "model.zero_grad()\n",
        "logits[0, target_category].backward()\n",
        "\n",
        "# Grad-CAM calculation\n",
        "# activations: (1, 8, 197, 768)\n",
        "# gradients: (1, 8, 197, 768)\n",
        "pooled_gradients = torch.mean(gradients, dim=[0, 1])\n",
        "weighted_activations = activations * pooled_gradients.unsqueeze(0).unsqueeze(0)\n",
        "heatmap = torch.mean(weighted_activations, dim=-1).squeeze().cpu().numpy()\n",
        "heatmap = heatmap[:, 1:]  # remove CLS token\n",
        "heatmap = heatmap.reshape(len(original_frames), 14, 14)\n",
        "heatmap_resized = np.array([cv2.resize(h, (224, 224)) for h in heatmap])\n",
        "heatmap_resized = np.array(\n",
        "    [(h - np.min(h)) / (np.max(h) - np.min(h) + 1e-8) for h in heatmap_resized]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM6apGPhp9CS",
        "outputId": "1cef1cf4-7482-4dbb-bfa4-56a3ebd10ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-e946a39fc440>:170: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
            "  plt.tight_layout()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Overlay and save each frame with Grad-CAM heatmap\n",
        "FRAME_PERCENTAGE = 0.6\n",
        "for idx in range(heatmap_resized.shape[0]):\n",
        "    frame = original_frames[idx]\n",
        "    heatmap_uint8 = np.uint8(255 * heatmap_resized[idx])\n",
        "    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
        "    overlay = cv2.addWeighted(\n",
        "        frame, FRAME_PERCENTAGE, heatmap_color, 1 - FRAME_PERCENTAGE, 0\n",
        "    )\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Frame {idx}\")\n",
        "    plt.axis(\"off\")\n",
        "    os.makedirs(\"cam\", exist_ok=True)\n",
        "    plt.savefig(f\"cam/cam_{idx}.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    if idx == heatmap_resized.shape[0] - 1:\n",
        "        fig, axes = plt.subplots(1, heatmap_resized.shape[0], figsize=(20, 4))\n",
        "        # colormap legend\n",
        "        cmap = plt.cm.jet\n",
        "        norm = plt.Normalize(vmin=0, vmax=1)\n",
        "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "        sm.set_array([])\n",
        "\n",
        "\n",
        "        cbar_ax = fig.add_axes([0.15, 0.15, 0.7, 0.02])\n",
        "        cbar = fig.colorbar(sm, cax=cbar_ax, orientation=\"horizontal\")\n",
        "        cbar.set_label(\"Activation Intensity\")\n",
        "\n",
        "        # plot\n",
        "        plt.subplots_adjust(bottom=0.2)\n",
        "        for i in range(heatmap_resized.shape[0]):\n",
        "            frame = original_frames[i]\n",
        "            hm_uint8 = np.uint8(255 * heatmap_resized[i])\n",
        "            hm_color = cv2.applyColorMap(hm_uint8, cv2.COLORMAP_JET)\n",
        "            frame_overlay = cv2.addWeighted(\n",
        "                frame, FRAME_PERCENTAGE, hm_color, 1 - FRAME_PERCENTAGE, 0\n",
        "            )\n",
        "            axes[i].imshow(cv2.cvtColor(frame_overlay, cv2.COLOR_BGR2RGB))\n",
        "            axes[i].set_title(f\"Frame {i}\")\n",
        "            axes[i].axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"cam/all_frames_strip.png\")\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "VEBGfoBGj7LO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}