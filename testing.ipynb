{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fbfcea-2874-48af-be67-7f719278124f",
   "metadata": {},
   "source": [
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcb1f405-4334-4af4-a1f3-ad581459b598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:13<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "#from src.sampling import Sampler, FixedStepSampler\n",
    "from src.augmentations import default_transforms, train_augmentations\n",
    "\n",
    "CATEGORY_INDEX = {\n",
    "    \"brush_hair\": 0,\n",
    "    \"cartwheel\": 1,\n",
    "    \"catch\": 2,\n",
    "    \"chew\": 3,\n",
    "    \"climb\": 4,\n",
    "    \"climb_stairs\": 5,\n",
    "    \"draw_sword\": 6,\n",
    "    \"eat\": 7,\n",
    "    \"fencing\": 8,\n",
    "    \"flic_flac\": 9,\n",
    "    \"golf\": 10,\n",
    "    \"handstand\": 11,\n",
    "    \"kiss\": 12,\n",
    "    \"pick\": 13,\n",
    "    \"pour\": 14,\n",
    "    \"pullup\": 15,\n",
    "    \"pushup\": 16,\n",
    "    \"ride_bike\": 17,\n",
    "    \"shoot_bow\": 18,\n",
    "    \"shoot_gun\": 19,\n",
    "    \"situp\": 20,\n",
    "    \"smile\": 21,\n",
    "    \"smoke\": 22,\n",
    "    \"throw\": 23,\n",
    "    \"wave\": 24,\n",
    "}\n",
    "\n",
    "\n",
    "class VideoDataCollator:\n",
    "    \"\"\"\n",
    "    Custom data collator for TimeSFormer.\n",
    "    Converts (clip, label) tuples into a dictionary format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        clips, labels = zip(*features)  # Unpack (clip, label)\n",
    "        batch = {\n",
    "            \"pixel_values\": torch.stack(clips),  # Stack clips into batch\n",
    "            \"labels\": torch.tensor(\n",
    "                labels, dtype=torch.long\n",
    "            ),  # Convert labels to tensor\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "\n",
    "def split_sources(dataset_path, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Splits source folders into train and val sets before processing clips.\n",
    "    Ensures that all clips from a source video stay in the same set.\n",
    "    \"\"\"\n",
    "    train_sources = {}\n",
    "    val_sources = {}\n",
    "\n",
    "    for category in os.listdir(dataset_path):  # Iterate over action categories\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "\n",
    "        instances = os.listdir(category_path)  # List all source folders (video IDs)\n",
    "        random.shuffle(instances)  # Shuffle instances before splitting\n",
    "\n",
    "        split_idx = int(len(instances) * train_ratio)\n",
    "        train_sources[category] = instances[:split_idx]  # First 80% for training\n",
    "        val_sources[category] = instances[split_idx:]  # Last 20% for validation\n",
    "\n",
    "    return train_sources, val_sources\n",
    "\n",
    "\n",
    "def create_clips(frames, clip_size=8, frame_paths=None):\n",
    "    \"\"\"\n",
    "    Given a list of sampled frames, create multiple [clip_size]-frame clips.\n",
    "    Each clip is returned as a tensor.\n",
    "    \"\"\"\n",
    "    clips = []\n",
    "    path_clips = []\n",
    "    \n",
    "    for i in range(0, len(frames) - clip_size + 1, clip_size):\n",
    "        clip = frames[i : i + clip_size]\n",
    "        if len(clip) == clip_size:\n",
    "            clips.append(torch.stack(clip))  # Convert the clip to a tensor\n",
    "            if frame_paths is not None:\n",
    "                path_clips.append(frame_paths[i : i + clip_size])\n",
    "\n",
    "    return clips, path_clips\n",
    "            \n",
    "            \n",
    "def process_dataset(\n",
    "    dataset_path,\n",
    "    sources_dict,\n",
    "    augmentation_transform=None,\n",
    "    sampler: Sampler = FixedStepSampler(),\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes dataset based on a predefined list of sources.\n",
    "    \"\"\"\n",
    "    if augmentation_transform is None:\n",
    "        augmentation_transform = lambda image: {\"image\": image}\n",
    "\n",
    "    dataset = []\n",
    "    dataset_w_paths = []\n",
    "\n",
    "    for category, instances in tqdm(sources_dict.items()):\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "\n",
    "        for instance in instances:\n",
    "            instance_path = os.path.join(category_path, instance)\n",
    "            if not os.path.isdir(instance_path):\n",
    "                # print(f\"Skipping non-directory file: {instance_path}\")\n",
    "                continue\n",
    "            \n",
    "            if isinstance(sampler, FrameSampler): \n",
    "                # Load sampled frames\n",
    "                frame_paths = sampler.sample(instance_path)\n",
    "                frames = []\n",
    "                for path in frame_paths:\n",
    "                    try:\n",
    "                        frames.append(\n",
    "                            default_transforms(\n",
    "                                image=augmentation_transform(\n",
    "                                    image=cv2.cvtColor(\n",
    "                                        cv2.imread(path), cv2.COLOR_BGR2RGB\n",
    "                                    )\n",
    "                                )[\"image\"]\n",
    "                            )[\"image\"]\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing frame {path}: {e}\")\n",
    "                        frames.append(None)\n",
    "\n",
    "            if isinstance(sampler, ClipSampler):\n",
    "                frames_created, frame_paths = sampler.sample(instance_path)\n",
    "                frame_paths = [f\"{category}_{instance}_{round(pos, 3)}\" for pos in frame_paths]\n",
    "                frames = []\n",
    "                for frame in frames_created:\n",
    "                    try:\n",
    "                        frames.append(\n",
    "                            default_transforms(\n",
    "                                image=augmentation_transform(\n",
    "                                    image=frame\n",
    "                                )[\"image\"]\n",
    "                            )[\"image\"]\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing frame {path}: {e}\")\n",
    "                        frames.append(None) \n",
    "                \n",
    "\n",
    "            # Create 8-frame clips\n",
    "            clips, clips_path = create_clips(frames, 8,frame_paths)\n",
    "\n",
    "            for idx, clip in enumerate(clips):\n",
    "                dataset.append((clip, CATEGORY_INDEX[category]))\n",
    "                if clips_path is not None:\n",
    "                    dataset_w_paths.append((clip, clips_path[idx], CATEGORY_INDEX[category]))\n",
    "\n",
    "    return dataset, dataset_w_paths  # List of (clip, label)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_PATH = \"../data/HMDB_simp_clean/\"\n",
    "    # print(len(FixedStepSampler.sample(frame_dir=\"../HMDB_simp/\")))\n",
    "    train_sources, val_sources = split_sources(DATASET_PATH)\n",
    "\n",
    "    # Process train and val sets separately\n",
    "    train_dataset,train_dataset_paths = process_dataset(\n",
    "        DATASET_PATH, train_sources, augmentation_transform=train_augmentations\n",
    "    )\n",
    "    a = train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "839e7e7d-4922-4dd3-a216-ada8a790bf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:45<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "train_dataset,train_dataset_paths = process_dataset(\n",
    "        DATASET_PATH, train_sources, augmentation_transform=train_augmentations, sampler = InterpolationSampler()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a933d360-3623-454a-8d42-fdfb4c1f9b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980,\n",
       " ['ride_bike_C0BC2937_0.0',\n",
       "  'ride_bike_C0BC2937_11.286',\n",
       "  'ride_bike_C0BC2937_22.571',\n",
       "  'ride_bike_C0BC2937_33.857',\n",
       "  'ride_bike_C0BC2937_45.143',\n",
       "  'ride_bike_C0BC2937_56.429',\n",
       "  'ride_bike_C0BC2937_67.714',\n",
       "  'ride_bike_C0BC2937_79.0'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset_paths), train_dataset_paths[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978e4e9-690d-4670-a4db-cc95837f87b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02ce26-de4d-40aa-ac3a-d5d57d0d82ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c061ea-eb13-4b9f-8d4f-321339a5faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/HMDB_simp_clean/\"\n",
    "sources_dict=train_sources\n",
    "augmentation_transform=train_augmentations\n",
    "sampler: Sampler = FixedStepSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801bdcbd-36bc-4a9f-8391-48e1f38753a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:00<00:00, 29.77it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "dataset_w_paths = []\n",
    "\n",
    "for category, instances in tqdm(sources_dict.items()):\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    \n",
    "    category_path = '../data/HMDB_simp_clean/pour'\n",
    "    instances=['DC13AD0D']\n",
    "\n",
    "    for instance in instances:\n",
    "        instance_path = os.path.join(category_path, instance)\n",
    "        if not os.path.isdir(instance_path):\n",
    "            # print(f\"Skipping non-directory file: {instance_path}\")\n",
    "            continue\n",
    "\n",
    "        # Load sampled frames\n",
    "        frame_paths = sampler.sample(instance_path)\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        for path in frame_paths:\n",
    "            try:\n",
    "                frames.append(\n",
    "                    default_transforms(\n",
    "                        image=augmentation_transform(\n",
    "                            image=cv2.cvtColor(\n",
    "                                cv2.imread(path), cv2.COLOR_BGR2RGB\n",
    "                            )\n",
    "                        )[\"image\"]\n",
    "                    )[\"image\"]\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {path}: {e}\")\n",
    "                frames.append(None)\n",
    "\n",
    "        # Create 8-frame clips\n",
    "        clips, clips_path = create_clips(frames, 8,frame_paths)\n",
    "\n",
    "        \n",
    "        for idx, clip in enumerate(clips):\n",
    "            dataset.append((clip, CATEGORY_INDEX[category]))\n",
    "            if clips_path is not None:\n",
    "                dataset_w_paths.append((clip, clips_path[idx], CATEGORY_INDEX[category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e2951-9369-4c01-b15a-e3eea7b2d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_clips(frames, clip_size=8, frame_paths=None):\n",
    "\"\"\"\n",
    "Given a list of sampled frames, create multiple [clip_size]-frame clips.\n",
    "Each clip is returned as a tensor.\n",
    "\"\"\"\n",
    "clips = []\n",
    "path_clips = []\n",
    "clip_size=8\n",
    "\n",
    "for i in range(0, len(frames) - clip_size + 1, clip_size):\n",
    "    clip = frames[i : i + clip_size]\n",
    "    if len(clip) == clip_size:\n",
    "        clips.append(torch.stack(clip))  # Convert the clip to a tensor\n",
    "        if frame_paths is not None:\n",
    "            #path_clips.append(frame_paths[i : i + clip_size])\n",
    "            #path_clips.extend(frame_paths[i:i + clip_size])\n",
    "            path_clips.append(frame_paths[i : i + clip_size])\n",
    "            #path_clips.append([path for path in frame_paths[i:i + clip_size]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f37d7-8217-4d1e-9d65-38ae148dd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_paths[876][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "37baece8-d712-4fa2-9dd5-add9f5160bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00d9b1e7-fa4d-4082-8d09-91a27ad35063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 191\n",
      "Clip paths: ['brush_hair_1C53D816_0.0', 'brush_hair_1C53D816_44.571', 'brush_hair_1C53D816_89.143', 'brush_hair_1C53D816_133.714', 'brush_hair_1C53D816_178.286', 'brush_hair_1C53D816_222.857', 'brush_hair_1C53D816_267.429', 'brush_hair_1C53D816_312.0']\n",
      "Label: 0\n",
      "Clip length: 8\n"
     ]
    }
   ],
   "source": [
    "def find_clips_by_substring(dataset_w_paths, substring):\n",
    "    \"\"\"\n",
    "    Prints clip info for any clip in dataset_w_paths where the path contains the given substring.\n",
    "    \n",
    "    Args:\n",
    "        dataset_w_paths (list): List of (clip, clip_paths, label) tuples.\n",
    "        substring (str): Substring to search for in the clip paths.\n",
    "    \"\"\"\n",
    "    for idx, (_, clip_paths, label) in enumerate(dataset_w_paths):\n",
    "        if any(substring in path for path in clip_paths):\n",
    "            print(f\"Index: {idx}\")\n",
    "            print(f\"Clip paths: {clip_paths}\")\n",
    "            print(f\"Label: {label}\")\n",
    "            print(f\"Clip length: {len(clip_paths)}\")\n",
    "\n",
    "find_clips_by_substring(train_dataset_paths, \"1C53D816\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d720cf-1c57-41c1-bedd-7ade13c42e29",
   "metadata": {},
   "source": [
    "### sampling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8181158c-a815-43fc-82c5-1bc71b59fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Sampler(ABC):\n",
    "    @abstractmethod\n",
    "    def sample(self, frame_dir=None, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def list_frames(frame_dir):\n",
    "        return [\n",
    "            os.path.join(frame_dir, file)\n",
    "            for file in sorted(os.listdir(frame_dir))\n",
    "            if file.endswith((\".jpg\", \".png\", \".jpeg\"))\n",
    "        ]\n",
    "\n",
    "class FrameSampler(Sampler):\n",
    "    #sub-parent class for selecting frame locations\n",
    "    @abstractmethod\n",
    "    def sample(self, frame_dir):\n",
    "        pass\n",
    "\n",
    "class ClipSampler(Sampler):\n",
    "    #sub-parent class for creating frames\n",
    "    @abstractmethod\n",
    "    def sample(self, frame_dir):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FixedStepSampler(FrameSampler):\n",
    "    def __init__(self, step=8):\n",
    "        self.step = step\n",
    "        \n",
    "    def sample(self, frame_dir):\n",
    "        \"\"\"\n",
    "        Load every [step]-th frame from a directory.\n",
    "        \"\"\"\n",
    "        frame_files = self.list_frames(frame_dir)\n",
    "        return frame_files[::self.step]\n",
    "\n",
    "\n",
    "class EquidistantSampler(FrameSampler):\n",
    "    def __init__(self, initial_offset=5, min_frames=8):\n",
    "        self.initial_offset = initial_offset\n",
    "        self.min_frames = min_frames\n",
    "        \n",
    "    def sample(self, frame_dir):\n",
    "        frame_files = self.list_frames(frame_dir)\n",
    "        total_frames = len(frame_files)\n",
    "        \n",
    "        if total_frames <= self.initial_offset:\n",
    "            return frame_files  # Not enough frames, return all\n",
    "\n",
    "        step = max(1, int((total_frames - self.initial_offset) / self.min_frames))\n",
    "        print(total_frames)\n",
    "        print(step)\n",
    "        return frame_files[self.initial_offset::step]\n",
    "\n",
    "\n",
    "class InterpolationSampler(ClipSampler):\n",
    "    \"\"\"\n",
    "    Sample frames from a video by interpolating between key frames.\n",
    "    Outputs interpolated frames as a numpy array (and frame positions for checking purposes).\n",
    "    \"\"\"\n",
    "    def __init__(self, min_frames=8):\n",
    "        self.min_frames = min_frames\n",
    "        self.transform = transforms.ToTensor()\n",
    "        \n",
    "    def sample(self, frame_dir):\n",
    "        frame_files = self.list_frames(frame_dir)\n",
    "        total_frames = len(frame_files)\n",
    "\n",
    "        if total_frames == 0:\n",
    "            raise ValueError(\"Video is empty\")\n",
    "        \n",
    "        else:\n",
    "            video = []\n",
    "            for f in frame_files:\n",
    "                frame = cv2.cvtColor(cv2.imread(f), cv2.COLOR_BGR2RGB)\n",
    "                video.append(frame)\n",
    "                \n",
    "            positions = np.linspace(0, total_frames - 1, self.min_frames)\n",
    "            clip = []\n",
    "            for pos in positions:\n",
    "                low_idx = int(np.floor(pos))\n",
    "                high_idx = min(low_idx + 1, total_frames - 1)\n",
    "                alpha = pos - low_idx\n",
    "\n",
    "                frame_low = video[low_idx]\n",
    "                frame_high = video[high_idx]\n",
    "                \n",
    "                interp_frame = (1 - alpha) * frame_low + alpha * frame_high\n",
    "                interp_frame = np.clip(interp_frame, 0, 255).astype(np.uint8)       \n",
    "                clip.append(interp_frame)\n",
    "                \n",
    "        clip_frames = np.stack(clip, axis=0)\n",
    "        positions_frames = list(positions)\n",
    "        \n",
    "        return clip_frames, positions_frames \n",
    "        \n",
    "class AugmentationSampler(ClipSampler):\n",
    "    \"\"\"\n",
    "    Sample frames from a video by adding new augmented frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_frames=8):\n",
    "        self.min_frames = min_frames\n",
    "        \n",
    "    def sample(self, frame_dir):\n",
    "        # TODO: Elisa-tea\n",
    "        frame_files = self.list_frames(frame_dir)\n",
    "        return frame_files  # Placeholder implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9edf912-1fef-4899-a29d-132bd0fb287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/HMDB_simp_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e12c39-a87e-40bf-a75e-f95e39921326",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = process_dataset(dataset_path, train_sources, augmentation_transform=None, sampler=FixedStepSampler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2bd906-c0c0-4f35-a510-3846d241e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63ab1c-f6d4-4b02-a284-7ad63cba30ef",
   "metadata": {},
   "source": [
    "### process_dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63672bad-c137-44ab-a12c-2cd608b7bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(\n",
    "    dataset_path,\n",
    "    sources_dict,\n",
    "    augmentation_transform=None,\n",
    "    sampler: Sampler = FixedStepSampler(),\n",
    "):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "abba26e6-e4bd-4b53-9fc4-5f35df167139",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/HMDB_simp_clean/\"\n",
    "sources_dict=train_sources\n",
    "augmentation_transform=train_augmentations\n",
    "sampler: Sampler = FixedStepSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7988292-5e82-459f-be68-bb36fb0571bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "291b276c-798a-47cd-b5fb-ea051e87ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation_transform is None:\n",
    "    augmentation_transform = lambda image: {\"image\": image}\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for category, instances in tqdm(sources_dict.items()):\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "\n",
    "        for instance in instances:\n",
    "            instance_path = os.path.join(category_path, instance)\n",
    "            if not os.path.isdir(instance_path):\n",
    "                # print(f\"Skipping non-directory file: {instance_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load sampled frames\n",
    "            frame_paths = sampler.sample(instance_path)\n",
    "\n",
    "            frames = []\n",
    "\n",
    "            for path in frame_paths:\n",
    "                try:\n",
    "                    frames.append(\n",
    "                        default_transforms(\n",
    "                            image=augmentation_transform(\n",
    "                                image=cv2.cvtColor(\n",
    "                                    cv2.imread(path), cv2.COLOR_BGR2RGB\n",
    "                                )\n",
    "                            )[\"image\"]\n",
    "                        )[\"image\"]\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing frame {path}: {e}\")\n",
    "                    frames.append(None)\n",
    "\n",
    "            # Create 8-frame clips\n",
    "            clips = create_clips(frames, 8)\n",
    "            for clip in clips:\n",
    "                dataset.append((clip, CATEGORY_INDEX[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d77cd6-efc4-482d-9903-0051115b2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames is a list of tensors of size X\n",
    "#clips creates n 8-frame clips\n",
    "print(len(frames)), print(frames[0].size()),len(clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be559f-a8b1-4776-a10c-0edcc5b12db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bcc5e8-35f0-4c22-a5a3-fb27e378a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6459af26-6402-417a-971a-6a381eb6c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a9603-f09f-4cb6-84ad-97393e2dfd0e",
   "metadata": {},
   "source": [
    "### InterpolationSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bfbef3-3e98-40fc-b95a-5226bd4c196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/HMDB_simp_clean/\"\n",
    "sources_dict=train_sources\n",
    "augmentation_transform=train_augmentations\n",
    "sampler: Sampler = FixedStepSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b1809ee-c990-4e46-88ae-4561cef7f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:00<00:00, 28.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset=[]\n",
    "dataset_w_paths=[]\n",
    "\n",
    "for category, instances in tqdm(sources_dict.items()):\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "        \n",
    "        category_path = '../data/HMDB_simp_clean/pour'\n",
    "        instances=['DC13AD0D']\n",
    "\n",
    "        for instance in instances:\n",
    "            instance_path = os.path.join(category_path, instance)\n",
    "            if not os.path.isdir(instance_path):\n",
    "                # print(f\"Skipping non-directory file: {instance_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load sampled frames\n",
    "            sampled = sampler.sample(instance_path)\n",
    "            frames = []\n",
    "\n",
    "            if isinstance(sampler, FrameSampler): \n",
    "\n",
    "                frame_paths = sampled\n",
    "                \n",
    "                for path in frame_paths:\n",
    "                    try:\n",
    "                        frames.append(\n",
    "                            default_transforms(\n",
    "                                image=augmentation_transform(\n",
    "                                    image=cv2.cvtColor(\n",
    "                                        cv2.imread(path), cv2.COLOR_BGR2RGB\n",
    "                                    )\n",
    "                                )[\"image\"]\n",
    "                            )[\"image\"]\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing frame {path}: {e}\")\n",
    "                        frames.append(None)\n",
    "    \n",
    "\n",
    "            if isinstance(sampler, ClipSampler):\n",
    "\n",
    "                clip_created = sampled\n",
    "\n",
    "                for frame in clip_created:\n",
    "                    try:\n",
    "                        frames.append(\n",
    "                            default_transforms(\n",
    "                                image=augmentation_transform(\n",
    "                                    frame\n",
    "                                )[\"image\"]\n",
    "                            )[\"image\"]\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing frame {path}: {e}\")\n",
    "                        frames.append(None)    \n",
    "                \n",
    "\n",
    "            # Create 8-frame clips\n",
    "            clips, clips_path = create_clips(frames, 8,frame_paths)\n",
    "\n",
    "            for idx, clip in enumerate(clips):\n",
    "                dataset.append((clip, CATEGORY_INDEX[category]))\n",
    "                if clips_path is not None:\n",
    "                    dataset_w_paths.append((clip, clips_path[idx], CATEGORY_INDEX[category]))\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a24454f1-f03a-4f62-9287-b323414cb85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, torch.Size([8, 3, 224, 224]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset),dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3551104-9def-4b59-8845-5164f2626540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_w_paths[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cef5d98-7ad2-4f89-b8c0-c9521ed41e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampler type: <class '__main__.FixedStepSampler'>, ClipSampler? False, FrameSampler? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sampler type: {type(sampler)}, ClipSampler? {isinstance(sampler, ClipSampler)}, FrameSampler? {isinstance(sampler, FrameSampler)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b58de50-a5a9-4dc9-8e86-0e2560a032e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_frames(frame_dir):\n",
    "        return [\n",
    "            os.path.join(frame_dir, file)\n",
    "            for file in sorted(os.listdir(frame_dir))\n",
    "            if file.endswith((\".jpg\", \".png\", \".jpeg\"))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c867aaf5-815d-4868-a2e8-553215b823bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "41e47b26-7fa8-4506-9c8e-017feeae890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 25/25 [00:00<00:00, 2899.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for category, instances in tqdm(sources_dict.items()):\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "        \n",
    "        category_path = '../data/HMDB_simp_clean/pour'\n",
    "        #instances=['DC13AD0D']\n",
    "\n",
    "        for instance in instances:\n",
    "            instance_path = os.path.join(category_path, instance)\n",
    "            if not os.path.isdir(instance_path):\n",
    "                # print(f\"Skipping non-directory file: {instance_path}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3063072-8e03-4c7c-ae6b-eb80d1d5c9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DC13AD0D'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b2fb2c6-9cd1-47d3-b386-38d250cdcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_files = list_frames(instance_path)\n",
    "total_frames = len(frame_files)\n",
    "\n",
    "if total_frames == 0:\n",
    "    raise ValueError(\"Video is empty\")\n",
    "\n",
    "else:\n",
    "    video = []\n",
    "    for f in frame_files:\n",
    "        frame = cv2.cvtColor(cv2.imread(f), cv2.COLOR_BGR2RGB)\n",
    "        video.append(frame)\n",
    "\n",
    "    positions = np.linspace(0, total_frames - 1, 8)\n",
    "    clip = []\n",
    "    for pos in positions:\n",
    "        low_idx = int(np.floor(pos))\n",
    "        high_idx = min(low_idx + 1, total_frames - 1)\n",
    "        alpha = pos - low_idx\n",
    "\n",
    "        frame_low = video[low_idx]\n",
    "        frame_high = video[high_idx]\n",
    "\n",
    "        interp_frame = (1 - alpha) * frame_low + alpha * frame_high\n",
    "        interp_frame = np.clip(interp_frame, 0, 255).astype(np.uint8)\n",
    "        clip.append(interp_frame)\n",
    "\n",
    "clip_final = np.stack(clip, axis=0)\n",
    "position_list=list(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa50bc9d-2357-47ff-abed-888d8c3d1d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,  41.,  82., 123., 164., 205., 246., 287.]), 41.0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions, (total_frames-1)/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ae4fae3c-4f9b-43d4-b1a1-e6fd105f1198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 41.0, 82.0, 123.0, 164.0, 205.0, 246.0, 287.0]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1acde8e8-008b-42c8-b6f3-c5748b709d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287, 287)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_idx,high_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8ecfd635-48b1-4e27-a151-e9ba7abc3033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, NoneType)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clip_final),type(frame_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c4344aea-9340-456c-8800-682ac4581b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in clip_final:\n",
    "    image = augmentation_transform(image=frame)[\"i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5b4ae89c-a6d2-450f-9227-3a16de2605ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_created = clip_final\n",
    "transformed_clip = []\n",
    "\n",
    "for frame in clip_created:\n",
    "    try:\n",
    "        transformed_clip.append(\n",
    "            default_transforms(\n",
    "                image=augmentation_transform(\n",
    "                    image=frame\n",
    "                )[\"image\"]\n",
    "            )[\"image\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {path}: {e}\")\n",
    "        transformed_clip.append(None)    \n",
    "\n",
    "clips,clips_path = create_clips(transformed_clip, 8,position_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966e8f5-5dd2-42a8-9275-d823a1a54f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f2b4d183-871e-488d-be7c-e898d1544e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "dataset_w_paths=[]\n",
    "\n",
    "for idx, clip in enumerate(clips):    \n",
    "    dataset.append((clip, CATEGORY_INDEX[category]))\n",
    "    if clips_path is not None:\n",
    "        dataset_w_paths.append((clip, clips_path[idx], CATEGORY_INDEX[category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8d69b68d-ff7b-4be2-97b6-59fcffe93110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 21)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_w_paths),dataset_w_paths[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5b9ddccc-d225-489b-913d-af652c4190df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transformed_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "23f86206-8de7-4bfd-a6ee-7e810109fc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, torch.Size([3, 224, 224]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transformed_clip),transformed_clip[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d095a006-2b19-40a3-a566-ca022047e911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "02d0df21-1d8f-4700-b3a9-a6fc3f1e5b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([8, 3, 224, 224]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clips),clips[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "82448ef0-61ca-4013-a988-3b4f2b0417c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 59,   9,   2],\n",
       "         [ 82,  29,  21],\n",
       "         [119,  65,  53],\n",
       "         ...,\n",
       "         [118,  21,   4],\n",
       "         [121,  19,   4],\n",
       "         [121,  19,   4]],\n",
       "\n",
       "        [[ 82,  29,  23],\n",
       "         [ 64,  11,   3],\n",
       "         [ 90,  36,  24],\n",
       "         ...,\n",
       "         [122,  22,   6],\n",
       "         [123,  21,   6],\n",
       "         [123,  21,   6]],\n",
       "\n",
       "        [[104,  49,  42],\n",
       "         [ 68,  14,   4],\n",
       "         [ 71,  17,   5],\n",
       "         ...,\n",
       "         [124,  25,   6],\n",
       "         [126,  25,   7],\n",
       "         [126,  25,   7]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[143,  99,  86],\n",
       "         [142,  98,  85],\n",
       "         [143, 100,  84],\n",
       "         ...,\n",
       "         [ 10,   4,   4],\n",
       "         [  9,   5,   2],\n",
       "         [  7,   6,   1]],\n",
       "\n",
       "        [[160, 114, 101],\n",
       "         [157, 113, 100],\n",
       "         [153, 110,  94],\n",
       "         ...,\n",
       "         [  9,   4,   1],\n",
       "         [  8,   5,   0],\n",
       "         [  8,   5,   0]],\n",
       "\n",
       "        [[160, 114, 101],\n",
       "         [156, 110,  97],\n",
       "         [141,  98,  82],\n",
       "         ...,\n",
       "         [  9,   4,   1],\n",
       "         [  8,   5,   0],\n",
       "         [  8,   5,   0]]],\n",
       "\n",
       "\n",
       "       [[[ 57,  14,   7],\n",
       "         [ 65,  22,  13],\n",
       "         [109,  62,  52],\n",
       "         ...,\n",
       "         [122,  23,   2],\n",
       "         [123,  24,   3],\n",
       "         [124,  25,   4]],\n",
       "\n",
       "        [[ 75,  32,  23],\n",
       "         [ 58,  14,   5],\n",
       "         [ 85,  41,  28],\n",
       "         ...,\n",
       "         [124,  25,   4],\n",
       "         [125,  26,   5],\n",
       "         [126,  27,   6]],\n",
       "\n",
       "        [[ 93,  49,  38],\n",
       "         [ 65,  21,   8],\n",
       "         [ 68,  24,  11],\n",
       "         ...,\n",
       "         [128,  28,   5],\n",
       "         [128,  29,   8],\n",
       "         [129,  30,   9]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 26,   5,  12],\n",
       "         [ 25,   4,  11],\n",
       "         [ 24,   3,  10],\n",
       "         ...,\n",
       "         [ 11,   3,   1],\n",
       "         [  9,   3,   3],\n",
       "         [  9,   3,   3]],\n",
       "\n",
       "        [[ 25,   5,  14],\n",
       "         [ 24,   5,  11],\n",
       "         [ 23,   4,  10],\n",
       "         ...,\n",
       "         [ 11,   3,   1],\n",
       "         [  9,   3,   3],\n",
       "         [  9,   3,   5]],\n",
       "\n",
       "        [[ 17,   0,   8],\n",
       "         [ 18,   1,   9],\n",
       "         [ 20,   1,   7],\n",
       "         ...,\n",
       "         [ 10,   2,   0],\n",
       "         [  8,   2,   4],\n",
       "         [  8,   2,   4]]],\n",
       "\n",
       "\n",
       "       [[[ 61,  23,  10],\n",
       "         [ 71,  31,  21],\n",
       "         [119,  76,  70],\n",
       "         ...,\n",
       "         [128,  32,   8],\n",
       "         [129,  33,   9],\n",
       "         [130,  34,  10]],\n",
       "\n",
       "        [[ 75,  33,  19],\n",
       "         [ 66,  24,  12],\n",
       "         [ 94,  49,  43],\n",
       "         ...,\n",
       "         [129,  33,   9],\n",
       "         [131,  35,  11],\n",
       "         [132,  36,  12]],\n",
       "\n",
       "        [[101,  54,  38],\n",
       "         [ 76,  30,  15],\n",
       "         [ 77,  29,  19],\n",
       "         ...,\n",
       "         [134,  35,  12],\n",
       "         [135,  36,  13],\n",
       "         [136,  37,  14]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[127,  87,  87],\n",
       "         [132,  94,  93],\n",
       "         [142, 102, 100],\n",
       "         ...,\n",
       "         [ 25,   0,   0],\n",
       "         [ 22,   0,   3],\n",
       "         [ 23,   4,   8]],\n",
       "\n",
       "        [[165, 123, 125],\n",
       "         [163, 123, 123],\n",
       "         [159, 118, 116],\n",
       "         ...,\n",
       "         [ 26,   0,   0],\n",
       "         [ 23,   1,   4],\n",
       "         [ 23,   4,   8]],\n",
       "\n",
       "        [[180, 138, 140],\n",
       "         [171, 129, 130],\n",
       "         [152, 110, 111],\n",
       "         ...,\n",
       "         [ 27,   1,   0],\n",
       "         [ 24,   2,   5],\n",
       "         [ 23,   4,   8]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 56,  21,  17],\n",
       "         [ 68,  31,  22],\n",
       "         [118,  79,  62],\n",
       "         ...,\n",
       "         [133,  33,   7],\n",
       "         [134,  34,   8],\n",
       "         [134,  34,   8]],\n",
       "\n",
       "        [[ 67,  30,  24],\n",
       "         [ 63,  25,  16],\n",
       "         [ 94,  52,  36],\n",
       "         ...,\n",
       "         [134,  34,   8],\n",
       "         [135,  35,   9],\n",
       "         [135,  35,   9]],\n",
       "\n",
       "        [[ 92,  49,  40],\n",
       "         [ 76,  34,  22],\n",
       "         [ 70,  27,  11],\n",
       "         ...,\n",
       "         [136,  36,  10],\n",
       "         [136,  36,  10],\n",
       "         [136,  36,  10]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[153, 120, 111],\n",
       "         [146, 113, 104],\n",
       "         [150, 117, 108],\n",
       "         ...,\n",
       "         [ 27,   2,   0],\n",
       "         [ 24,   3,   2],\n",
       "         [ 22,   4,   4]],\n",
       "\n",
       "        [[167, 134, 127],\n",
       "         [165, 132, 125],\n",
       "         [162, 129, 120],\n",
       "         ...,\n",
       "         [ 27,   2,   0],\n",
       "         [ 24,   3,   2],\n",
       "         [ 22,   3,   5]],\n",
       "\n",
       "        [[172, 139, 132],\n",
       "         [170, 137, 130],\n",
       "         [151, 118, 109],\n",
       "         ...,\n",
       "         [ 27,   2,   0],\n",
       "         [ 24,   2,   4],\n",
       "         [ 22,   3,   5]]],\n",
       "\n",
       "\n",
       "       [[[ 60,  21,  16],\n",
       "         [ 73,  32,  26],\n",
       "         [118,  76,  64],\n",
       "         ...,\n",
       "         [133,  34,   5],\n",
       "         [135,  36,   7],\n",
       "         [136,  37,   8]],\n",
       "\n",
       "        [[ 77,  33,  30],\n",
       "         [ 64,  21,  14],\n",
       "         [ 87,  43,  32],\n",
       "         ...,\n",
       "         [132,  33,   4],\n",
       "         [134,  35,   6],\n",
       "         [135,  36,   7]],\n",
       "\n",
       "        [[104,  55,  51],\n",
       "         [ 72,  25,  17],\n",
       "         [ 70,  22,  10],\n",
       "         ...,\n",
       "         [134,  33,   5],\n",
       "         [136,  35,   7],\n",
       "         [136,  35,   7]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[151, 118, 103],\n",
       "         [142, 109,  94],\n",
       "         [147, 114,  99],\n",
       "         ...,\n",
       "         [ 22,   1,   0],\n",
       "         [ 20,   5,   0],\n",
       "         [ 17,   7,   0]],\n",
       "\n",
       "        [[173, 137, 125],\n",
       "         [162, 126, 112],\n",
       "         [161, 125, 111],\n",
       "         ...,\n",
       "         [ 21,   0,   0],\n",
       "         [ 19,   4,   0],\n",
       "         [ 17,   4,   0]],\n",
       "\n",
       "        [[176, 140, 128],\n",
       "         [176, 140, 128],\n",
       "         [155, 119, 105],\n",
       "         ...,\n",
       "         [ 20,   0,   0],\n",
       "         [ 20,   2,   0],\n",
       "         [ 16,   3,   0]]],\n",
       "\n",
       "\n",
       "       [[[ 64,  24,  12],\n",
       "         [ 72,  32,  20],\n",
       "         [122,  78,  65],\n",
       "         ...,\n",
       "         [136,  38,  13],\n",
       "         [138,  38,  12],\n",
       "         [139,  39,  13]],\n",
       "\n",
       "        [[ 74,  32,  20],\n",
       "         [ 71,  29,  15],\n",
       "         [ 90,  46,  33],\n",
       "         ...,\n",
       "         [135,  37,  12],\n",
       "         [137,  37,  11],\n",
       "         [137,  37,  11]],\n",
       "\n",
       "        [[ 99,  55,  42],\n",
       "         [ 76,  32,  19],\n",
       "         [ 69,  23,   8],\n",
       "         ...,\n",
       "         [137,  37,  13],\n",
       "         [137,  37,  11],\n",
       "         [136,  36,  10]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[146, 114, 101],\n",
       "         [138, 106,  93],\n",
       "         [144, 112,  99],\n",
       "         ...,\n",
       "         [ 25,   1,   0],\n",
       "         [ 24,   7,   0],\n",
       "         [ 20,   7,   0]],\n",
       "\n",
       "        [[171, 139, 124],\n",
       "         [164, 132, 117],\n",
       "         [161, 129, 116],\n",
       "         ...,\n",
       "         [ 26,   1,   0],\n",
       "         [ 26,   9,   1],\n",
       "         [ 21,   8,   0]],\n",
       "\n",
       "        [[172, 140, 125],\n",
       "         [171, 139, 124],\n",
       "         [150, 118, 105],\n",
       "         ...,\n",
       "         [ 26,   1,   0],\n",
       "         [ 27,  10,   2],\n",
       "         [ 20,   7,   0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_clips(frames, clip_size=8, frame_paths=None):\n",
    "    \"\"\"\n",
    "    Given a list of sampled frames, create multiple [clip_size]-frame clips.\n",
    "    Each clip is returned as a tensor.\n",
    "    \"\"\"\n",
    "    clips = []\n",
    "    path_clips = []\n",
    "    \n",
    "    for i in range(0, len(frames) - clip_size + 1, clip_size):\n",
    "        clip = frames[i : i + clip_size]\n",
    "        if len(clip) == clip_size:\n",
    "            clips.append(torch.stack(clip))  # Convert the clip to a tensor\n",
    "            if frame_paths is not None:\n",
    "                path_clips.append(frame_paths[i : i + clip_size])\n",
    "\n",
    "    if frame_paths is not None:\n",
    "        return clips, path_clips\n",
    "    else:\n",
    "        return clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0235e8f-8fa1-4d21-92e6-f27fb29bb92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "347e84e3-cce2-49f3-95e4-9e954e702fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 320, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89cc8fa-4598-48a7-a856-35861725a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# 1. List frames manually\n",
    "frame_files = sorted([\n",
    "    os.path.join(instance_path, f)\n",
    "    for f in os.listdir(instance_path)\n",
    "    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "])\n",
    "\n",
    "print(f\"Found {len(frame_files)} frames\")\n",
    "\n",
    "total_frames = len(frame_files)\n",
    "\n",
    "if total_frames == 0:\n",
    "    raise ValueError(\"No frames found in the directory!\")\n",
    "\n",
    "# 2. Load frames manually with cv2\n",
    "video = []\n",
    "for f in frame_files:\n",
    "    try:\n",
    "        frame = cv2.imread(f)\n",
    "        if frame is None:\n",
    "            raise ValueError(f\"Frame {f} could not be read (maybe corrupted?)\")\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "        video.append(frame_tensor)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading frame {f}: {e}\")\n",
    "\n",
    "print(f\"Successfully loaded {len(video)} frames\")\n",
    "\n",
    "# 3. Interpolation positions\n",
    "min_frames = 8  # or whatever you want\n",
    "positions = torch.linspace(0, total_frames - 1, steps=min_frames)\n",
    "print(f\"Interpolation positions: {positions}\")\n",
    "\n",
    "# 4. Interpolate\n",
    "clip = []\n",
    "for pos in positions:\n",
    "    low_idx = int(torch.floor(pos).item())\n",
    "    high_idx = min(low_idx + 1, total_frames - 1)\n",
    "    alpha = pos - low_idx\n",
    "\n",
    "    frame_low = video[low_idx]\n",
    "    frame_high = video[high_idx]\n",
    "\n",
    "    interp_frame = (1 - alpha) * frame_low + alpha * frame_high\n",
    "    clip.append(interp_frame)\n",
    "\n",
    "clip_final = torch.stack(clip)\n",
    "print(f\"Final clip tensor shape: {clip_final.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0106e05-da5e-43cd-b625-78a9eef54910",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_final[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89975d-af36-47fe-a43e-588e3aeaab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(sources_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045544d9-5682-4f81-899a-0b8b7d37a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"../data/HMDB_simp_clean/brush_hair/020E3BBA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca830c4-5866-4935-951a-a8dadcbf6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c462475-6dcc-4e2f-ac4a-fe5fd4995f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dda5d3-55ba-4e20-964a-2d1b2d74cd46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
