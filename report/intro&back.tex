\documentclass[conference]{IEEEtran}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}

\begin{document}

\section{Introduction}
Understanding the actions of others has long been fundamental to human cognition. For early humans, recognising actions was a matter of survival — crucial for detecting threats and seizing opportunities. In the modern world, the ability to interpret the behaviour of others remains essential, underpinning technologies that drive social interaction, safety, and automation.
With the rise of artificial intelligence, machines are increasingly tasked with interpreting human actions across various domains, including robotics, security, healthcare, and human-computer interaction. This raises a central question: how can a machine define what constitutes an "action"?
Action recognition in videos presents a unique challenge due to variations in motion, background clutter, and the need to capture temporal dependencies. Traditional methods have struggled with these challenges, highlighting the need for more robust models. Vision Transformers (ViTs), which have recently demonstrated considerable promise, offer a potential solution. Unlike convolutional neural networks (CNNs), ViTs use self-attention mechanisms to model long-range dependencies, making them particularly suited for capturing complex spatiotemporal relationships in video data.
This project addresses the challenge of video-based action recognition using cutting-edge machine learning techniques, specifically Vision Transformers. The research aims to explore the efficacy of ViTs in action recognition, focusing on their ability to capture both spatial and temporal patterns without relying on traditional convolutional architectures.
The structure of the paper is as follows: Chapter II provides the background, Chapter III details the methodology, Chapter IV presents the experiments, Chapter V discusses the conclusion and related work, and Chapter VI contains the appendices and supplementary material.



\section{Background and Literature Review}

Action recognition begins with the analysis of video data, which can be defined as a temporally ordered sequence of frames. Within this framework, an action may be conceptualised as a semantically coherent region of pixels undergoing affine transformations across consecutive frames, with only minimal displacement between them. This limited displacement introduces significant redundancy, which must be addressed to enable efficient video processing.
Traditional methods, such as optical flow [FlowNet], sought to explicitly model motion, while techniques like temporal sampling were employed to reduce redundant information. These foundational approaches paved the way for the development of deep learning architectures that capture both spatial and temporal dynamics.
Meanwhile, other studies, such as [Learning Temporally Invariant and Localisable Features via Data Augmentation for Video Recognition], highlighted the importance of data augmentation. The motivation stems from the limited availability of labelled video data and the risk of overfitting large-scale datasets. To combat these issues, various augmentations—such as translation, rotation, scaling, and photometric perturbations like brightness adjustments—have been proposed, including strategies specifically targeting temporal robustness.
A significant advancement in the field came with the introduction of Transformer-based architectures, which had already demonstrated success in image analysis. The Vision Transformer (ViT) [An Image is Worth 16 by 16 Words: Transformers for Image recognition at scale], originally developed for image classification, proved that self-attention mechanisms alone—without convolutional operations—could effectively model spatial dependencies.
R3d review placeholder 
Despite notable progress in video recognition, there remains ongoing debate regarding which computational paradigms are best suited for achieving human-level perception. Prominent figures such as Yann LeCun have advocated a move away from conventional generative and probabilistic models towards joint-embedding architectures and energy-based learning. These perspectives are reflected in innovations such as TimeSformer [TimeSFormer], which extends ViT into the temporal domain. TimeSformer shows that spatiotemporal feature learning can be achieved directly from frame-level patches via self-attention, completely omitting convolutional layers. This approach preserves semantically consistent regions, enables precise classification, and outperforms conventional CNN-based models on large-scale video datasets.
TimeSformer adopts divided space-time attention and is pretrained on ImageNet before being fine-tuned on datasets such as Kinetics-400 and Something-Something v2 (SSv2). This training regime yielded state-of-the-art results, including a top-1 accuracy of 78% on Kinetics-400—a benchmark comprising 240,000 training videos and 20,000 validation videos across 400 human action classes. In contrast, our experiments use the HMDB-simp dataset, consisting of 25 classes and 1250 videos. While significantly smaller in scale, it provides sufficient variability to support prototyping and focused evaluation under limited-resource settings. Based on these outcomes, TimeSformer was selected as the core architecture in this study for action recognition on the HMDB_simp dataset. 
A key empirical insight from TimeSformer’s evaluation is that increasing the length of the input clip consistently enhances classification accuracy. In practice, video clips are randomly sampled from full-length videos at a frame rate of 1/32. The model is trained with a batch size of 16 using synchronised stochastic gradient descent (SGD) across 32 GPUs, with a momentum of 0.9 and a weight decay of 0.0001. These implementation details contribute significantly to the model’s overall performance.
Nonetheless, Transformer-based models often lack interpretability. This has driven the growing interest in Explainable AI (XAI) techniques such as Grad-CAM [GradCam], which aim to reveal the input regions most influential in a model’s decision-making. Grad-CAM provides visual explanations for predictions made by a broad class of CNN-based models by using the gradients of any target concept flowing into the final convolutional layer to generate a localisation map. This map highlights the critical regions in the input image, thereby facilitating greater trust, transparency, and debugging in AI systems.


\end{document}
